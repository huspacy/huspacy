params desc:
    - batch_size: x minta után indul a backpropagation, tehát ha 100 a size, akkor 100 minta után (100 példa a fájlban vagyis sor)
    fogja állítani a súlyokat
    - epoch: hányszor menjen végig az adatokon (az összesen), tehát 2 epoch akkor kétszer fog végig menni az adatokon

    - ha 1000 betanítási példa van, és a batch_size 500, akkor 2 iterációt vesz igénybe 1 epoch teljesítéshez

    - hidden_width: rejtett rétegek hossza (?)
    - maxout_pieces: hány egyenes alkossa az aktivációs függvényt, minnél több egyenes, annál bonyolultabb függvény (kvázi tanítható)

    - dropout: elfelejtettünk adott %-nyi neuron-t így elérve, hogy jobb legyen az általánosító képessége
    - L2: túltanulás és alul tanulás elkerüléséhez jól működik
            https://miro.medium.com/max/1400/0*tATGj-F5jlQU80GE.png

    - patience: ennyi lépés után nincs változás, akkor áll le
    - max_steps: annyi a max lépés, ha eléri leáll


model_path: best_hyperparams
    components.ner.model.extra_state_tokens: true
    components.ner.model.hidden_width: 64
    components.ner.model.maxout_pieces: 3
    training.dropout: 0.5
    training.optimizer.L2: 0.1
    training.optimizer.learn_rate.initial_rate: 0.00005

    train and dev test -> bias or variance error


ner:
dropout: 0.5
L2: 0.1
tokens: true
hidden width: 64
maxout pieces: 3

tagger:
dropout: 0.3
L2: 0.001
top_k = 2

parser:
?


NOTE
parser nem tud tanulni az szegedcorpuson

ötlet:
parseren kívül mindent UD+SZC
   - parsert meg akkor tesztelve csak a transformert tovább adva
   (kvázi mindent freeze-elni)
   - mindent az UD-n - > kuka: mindenhol ront


TODO:
batch_size: done
tagger fine-tune done (senter nélkül)
parser: Biaffine parsers test, aztán mehet a parser sweep
(mindent sweepelni -> Gyuri seaborn-os scriptje)



Biaffine parser
parser:
https://github.com/explosion/spacy-experimental/blob/master/projects/biaffine_parser/configs/base-config.cfg


models for assemble:
tagger_lemma_4 (nincs még fine-tune)
parser_4 -> with (pretrainelt taggerre) tagger_lemma_4
pretrain_train_ner_patience_evalfreq (pretrainelt taggerrel) tagger_lemma_4 dev: 93.11 test: 92.40



**************
biaffine, senter ok modelhez
tagger_lemma_5 (inprog) fine-tuneolt
biaffine_sent (inprog) nincs még fine-tune
pretrain_train_ner_patience_evalfreq (pretrainelt taggerrel) dev: 93.11 test: 92.40
**************



---------------------------------------------------------------------------------------------


mindegyikre igaz: Mindegyiket hagytam tovább tanulni és nagyobb iterációkat hagytam, így első ránézésre jobbak lettek a számok.
batch_size-t ksiebbre (128, 256, ezek voltak a defaultok)
eval_frequency-t nagyobbra (1000)
patience-t nagyobbra (20000)

ner: fine-tune-olt, pretrainelt tagger-rel (kisebb volt a szórása a iterációknak, tehát ilyen 91-92 között mozgott, nélküle meg 89-92)
    dev: 93.11
    test: 92.40


tagger: Még nincs fine-tune (jelenleg fut), de elnézve a sweepeket nagyon max tizedes javulások lehetnek itt-ott.
    sents:
        dev: 100
        test: 100
    tag:
        dev: 98.31
        test: 98.20
    pos:
        dev: 98.22
        test: 98.01
    morph:
        dev: 96.32
        test: 96.84
    lemma:
        dev: 97.74
        test: 97.73

parser: Még nincs fine-tune, és még nincs tesztelve azzal, amit Zsolti talált Biaffine parser-rel, de Zsolti futtatása alapján jobbnak fog igérkezni.
        Illetve tesztelni fogom még hogy milyen, ha megkapja a tagger modelt alapnak (többit komponenst nem, mert azok csak romlanak.
    uas:
        dev: 91.27
        test: 91.39
    las:
        dev: 85.24
        test: 85.76

nlp framework
 - general intro
 - spacy
komponensek
adatbázisok
gépi tanulás
 - introduction
 - ...
 - deep learning
 - tok2vec
 - transformer
related work
komponensek algoritmikai kifejtése

eredmények
 - kiértékelési metrikák
 - tok2vec vs transformer
 - hyperparameter optimization
 - parser (biaffine)
 - ner (beamner)
    - diff (ipymarkup)
??? külön error analysis, ahol csinálunk pl tagging error analysis-t is, pl összehasonlítjuk, az pos tag-enkénti f értékeket
conclusion

nerkor 1.41 ??





nlp
miért transzformer
hogyan jutottunk el a transzformerig (larg)

korpuszok
SZC, UD, NerKor, SzegedNER, wiki trf-en futtatni

komponensek bemutatása:
morph
sents
pos
parser, transfer learning + ner, (multitasking)
lemma
ner, encode: CNN, BiLSTM, embed: character, word alakú

hyperparams (?)

larg-model
diff
transformer: (belseje)
parser: deafault és biaffine
ner: default, beam_ner

accuaries
ipymark




