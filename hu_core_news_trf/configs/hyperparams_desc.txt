params desc:
    - batch_size: x minta után indul a backpropagation, tehát ha 100 a size, akkor 100 minta után (100 példa a fájlban vagyis sor)
    fogja állítani a súlyokat
    - epoch: hányszor menjen végig az adatokon (az összesen), tehát 2 epoch akkor kétszer fog végig menni az adatokon

    - ha 1000 betanítási példa van, és a batch_size 500, akkor 2 iterációt vesz igénybe 1 epoch teljesítéshez

    - hidden_width: rejtett rétegek hossza (?)
    - maxout_pieces: hány egyenes alkossa az aktivációs függvényt, minnél több egyenes, annál bonyolultabb függvény (kvázi tanítható)

    - dropout: elfelejtettünk adott %-nyi neuron-t így elérve, hogy jobb legyen az általánosító képessége
    - L2: túltanulás és alul tanulás elkerüléséhez jól működik
            https://miro.medium.com/max/1400/0*tATGj-F5jlQU80GE.png

    - patience: ennyi lépés után nincs változás, akkor áll le
    - max_steps: annyi a max lépés, ha eléri leáll


model_path: best_hyperparams
    components.ner.model.extra_state_tokens: true
    components.ner.model.hidden_width: 64
    components.ner.model.maxout_pieces: 3
    training.dropout: 0.5
    training.optimizer.L2: 0.1
    training.optimizer.learn_rate.initial_rate: 0.00005

    train and dev test -> bias or variance error


ner:
dropout: 0.5
L2: 0.1
tokens: true
hidden width: 64
maxout pieces: 3


NOTE
parser nem tud tanulni az szegedcorpuson

ötlet:
parseren kívül mindent UD+SZC
   - parsert meg akkor tesztelve csak a transformert tovább adva
   (kvázi mindent freeze-elni)
   - mindent az UD-n - > kuka: mindenhol ront


TODO:
batch_size: 128 -> test then done the ner
tagger fine-tune
parser: Biaffine parsers test, aztán mehet a parser sweep
(mindent sweepelni -> Gyuri seaborn-os scriptje)



Biaffine parser
parser:
https://github.com/explosion/spacy-experimental/blob/master/projects/biaffine_parser/configs/base-config.cfg

parser -> arc_predicter and arc_labeler-re csere

models for assemble:
tagger_lemma_4 (nincs még fine-tune)
parser_3 (pretrain with freeze kell majd)
parser_4 -> tagger_lemma_4 pretrain-el (minden más eddig "freeze")
pretrain_train_ner_patience_evalfreq (pretrainelt taggerrel) dev: 93.11 test:

TODO: assemble-t megaskolni
TODO: Zsolti TDK-ját elkérni
TODO: a parsernél miért tanul a sents is, amikor nincs benne a pipeline-ban, automatikus?
TODO: Peti sablon latex-jét elkérni
TODO: az assemble-ben a sents, tagger, pos, morph, lemma-t melyik modelből kell kiszedni, ha úgy csinálom, ahogy
tehát hogy a tagger előtanítva, amit a parser megkap, de úgy, hogy csak az alap transformert kapja meg a parser
a többi komponenst nem (replace_listeners)




---------------------------------------------------------------------------------------------


mindegyikre igaz: Mindegyiket hagytam tovább tanulni és nagyobb iterációkat hagytam, így első ránézésre jobbak lettek a számok.
batch_size-t ksiebbre (128, 256, ezek voltak a defaultok)
eval_frequency-t nagyobbra (1000)
patience-t nagyobbra (20000)

ner: fine-tune-olt, pretrainelt tagger-rel (kisebb volt a szórása a iterációknak, tehát ilyen 91-92 között mozgott, nélküle meg 89-92)
    dev: 93.11
    test: 92.40


tagger: Még nincs fine-tune (jelenleg fut), de elnézve a sweepeket nagyon max tizedes javulások lehetnek itt-ott.
    sents:
        dev: 100
        test: 100
    tag:
        dev: 98.31
        test: 98.20
    pos:
        dev: 98.22
        test: 98.01
    morph:
        dev: 96.32
        test: 96.84
    lemma:
        dev: 97.74
        test: 97.73

parser: Még nincs fine-tune, és még nincs tesztelve azzal, amit Zsolti talált Biaffine parser-rel, de Zsolti futtatása alapján jobbnak fog igérkezni.
        Illetve tesztelni fogom még hogy milyen, ha megkapja a tagger modelt alapnak (többit komponenst nem, mert azok csak romlanak.
    uas:
        dev: 91.27
        test: 91.39
    las:
        dev: 85.24
        test: 85.76